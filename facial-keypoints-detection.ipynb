{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "import datetime\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "from scipy import ndimage\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.preprocessing import minmax_scale\n",
      "from sklearn.preprocessing import normalize\n",
      "from optparse import OptionParser\n",
      "import matplotlib.pyplot as plt\n",
      "import random"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('training.csv', 'rb') as f:\n",
      "    reader = csv.reader(f, delimiter=',', quoting=csv.QUOTE_NONE)\n",
      "    rows = list(reader)\n",
      "    headers = rows[0]\n",
      "    data = rows[1:]\n",
      "\n",
      "print \"Read data\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Read data\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "image_size = 96\n",
      "pixel_depth = 255.0\n",
      "dataset = np.ndarray(shape=(len(data), image_size, image_size), dtype=np.float32)\n",
      "labels = np.ndarray(shape=(len(data), 30), dtype=np.float32)\n",
      "\n",
      "index = 0\n",
      "for row in data:\n",
      "    try:\n",
      "        # Process labels (replace '' values with -1)\n",
      "        labels[index, :] = np.asarray(['-1' if v == '' else v for v in data[index][0:30]], dtype=np.float32)\n",
      "    except ValueError as e:\n",
      "        print e\n",
      "        print data[index]\n",
      "    try:\n",
      "        # Process image\n",
      "        image_str = data[index][30].split(' ')\n",
      "        image_data = np.asarray(image_str, dtype=np.float32).reshape((image_size, image_size))\n",
      "        image_data = (image_data - pixel_depth/2) / pixel_depth\n",
      "        if image_data.shape != (image_size, image_size):\n",
      "            raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
      "        dataset[index, :, :] = image_data\n",
      "        index += 1\n",
      "    except IOError as e:\n",
      "        print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
      "\n",
      "print \"Processed \" + str(index) + \" train images and labels\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 7049 train images and labels\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('test.csv', 'rb') as f:\n",
      "    reader = csv.reader(f, delimiter=',', quoting=csv.QUOTE_NONE)\n",
      "    rows = list(reader)\n",
      "    headers = rows[0]\n",
      "    data = rows[1:]\n",
      "\n",
      "test_dataset = np.ndarray(shape=(len(data), image_size, image_size), dtype=np.float32)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "index = 0\n",
      "for row in data:\n",
      "    try:\n",
      "        image_id = data[index][0]\n",
      "        # Process image\n",
      "        image_str = data[index][1].split(' ')\n",
      "        image_data = np.asarray(image_str, dtype=np.float32).reshape((image_size, image_size))\n",
      "        #image_data = (image_data - pixel_depth/2) / pixel_depth\n",
      "        if image_data.shape != (image_size, image_size):\n",
      "            raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
      "        test_dataset[index, :, :] = image_data\n",
      "        index += 1\n",
      "    except IOError as e:\n",
      "        print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
      "print \"Processed \" + str(index) + \" test images\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 1783 test images\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "valid_size = int(dataset.shape[0]/20)\n",
      "def randomize(dataset, labels):\n",
      "    np.random.seed(13) # Added seed to ensure replicability\n",
      "    permutation = np.random.permutation(labels.shape[0])\n",
      "    shuffled_dataset = dataset[permutation,:,:]\n",
      "    shuffled_labels = labels[permutation]\n",
      "    return shuffled_dataset, shuffled_labels\n",
      "\n",
      "dataset, labels = randomize(dataset, labels)\n",
      "valid_dataset, valid_labels = (dataset[0:valid_size], labels[0:valid_size])\n",
      "train_dataset, train_labels = (dataset[valid_size:], labels[valid_size:])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "random.seed(13)\n",
      "rand_num = random.randint(0,train_dataset.shape[0]-1)\n",
      "\n",
      "def test_images(dataset):\n",
      "    sample = dataset[rand_num,:,:] #Note: I am using the same rand_num as before for consistency.\n",
      "    plt.imshow(sample)\n",
      "    plt.show()\n",
      "\n",
      "test_images(train_dataset)\n",
      "\n",
      "print('Training:', train_dataset.shape, train_labels.shape)\n",
      "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
      "print('Testing:', test_dataset.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('Training:', (6697, 96, 96), (6697, 30))\n",
        "('Validation:', (352, 96, 96), (352, 30))\n",
        "('Testing:', (1783, 96, 96))\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Flatteing images\n",
      "\n",
      "train_dataset, train_labels = (train_dataset.reshape((6697, image_size * image_size)), train_labels)\n",
      "valid_dataset, valid_labels = (valid_dataset.reshape((352, image_size * image_size)), valid_labels)\n",
      "test_dataset = test_dataset.reshape((1783, image_size * image_size))\n",
      "print('Training:', train_dataset.shape, train_labels.shape)\n",
      "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
      "print('Testing:', test_dataset.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('Training:', (6697, 9216), (6697, 30))\n",
        "('Validation:', (352, 9216), (352, 30))\n",
        "('Testing:', (1783, 9216))\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# With gradient descent training, even this much data is prohibitive.\n",
      "# Subset the training data for faster turnaround.\n",
      "batch_size = 100\n",
      "num_labels = 30\n",
      "learning_rate = 0.005\n",
      "\n",
      "graph = tf.Graph()\n",
      "with graph.as_default():\n",
      "    # Input data.\n",
      "    # Load the training, validation and test data into constants that are\n",
      "    # attached to the graph.\n",
      "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
      "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
      "    tf_valid_dataset = tf.constant(valid_dataset)\n",
      "    #tf_test_dataset = tf.constant(test_dataset)\n",
      "\n",
      "    # Variables.\n",
      "    # These are the parameters that we are going to be training. The weight\n",
      "    # matrix will be initialized using random valued following a (truncated)\n",
      "    # normal distribution. The biases get initialized to zero.\n",
      "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
      "    biases = tf.Variable(tf.zeros([num_labels]))\n",
      "\n",
      "    # Training computation.\n",
      "    # We multiply the inputs with the weight matrix, and add biases. We compute\n",
      "    # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
      "    # it's very common, and it can be optimized). We take the average of this\n",
      "    # cross-entropy across all training examples: that's our loss.\n",
      "    output = tf.matmul(tf_train_dataset, weights) + biases\n",
      "    output = tf.nn.l2_normalize(output, 1)\n",
      "    output = (output + 1) / 2\n",
      "    output = output * image_size\n",
      "    loss = tf.nn.l2_loss(output - tf_train_labels)\n",
      "    #loss = tf.reduce_mean(tf.reduce_sum(tf.square(output - tf_train_labels), 1))\n",
      "\n",
      "    # Optimizer.\n",
      "    # We are going to find the minimum of this loss using gradient descent.\n",
      "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
      "\n",
      "    # Predictions for the training, validation, and test data.\n",
      "    # These are not part of training, but merely here so that we can report\n",
      "    # accuracy figures as we train.\n",
      "    train_prediction = output\n",
      "    valid_prediction = tf.matmul(tf_valid_dataset, weights) + biases\n",
      "    #test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
      "    print \"tf_train_dataset \", tf_train_dataset.get_shape().as_list()\n",
      "    print \"tf_train_labels \", tf_train_labels.get_shape().as_list()\n",
      "    print \"tf_valid_dataset \", tf_valid_dataset.get_shape().as_list()\n",
      "    print \"weights \", weights.get_shape().as_list()\n",
      "    print \"biases \", biases.get_shape().as_list()\n",
      "    print \"output \", output.get_shape().as_list()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tf_train_dataset  [100, 9216]\n",
        "tf_train_labels  [100, 30]\n",
        "tf_valid_dataset  [352, 9216]\n",
        "weights  [9216, 30]\n",
        "biases  [30]\n",
        "output  [100, 30]\n"
       ]
      }
     ],
     "prompt_number": 89
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_steps = 1200\n",
      "\n",
      "def accuracy(predictions, labels):\n",
      "    #print (np.argmax(predictions, 1))\n",
      "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
      "      / predictions.shape[0])\n",
      "\n",
      "def RMSE(predictions, labels):\n",
      "   return mean_squared_error(predictions, labels)**0.5\n",
      "\n",
      "with tf.Session(graph=graph) as session:\n",
      "  tf.initialize_all_variables().run()\n",
      "  print(\"Initialized\")\n",
      "  for step in range(num_steps):\n",
      "    # Pick an offset within the training data, which has been randomized.\n",
      "    # Note: we could use better randomization across epochs.\n",
      "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
      "    # Generate a minibatch.\n",
      "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
      "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
      "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
      "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
      "    # and the value is the numpy array to feed to it.\n",
      "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
      "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
      "    #print \"predictions \", predictions\n",
      "    if (step % 100 == 0):\n",
      "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
      "      print(\"Minibatch RMSE: %f\" % RMSE(predictions, batch_labels))\n",
      "      print(\"Validation RMSE: %f\" % RMSE(valid_prediction.eval(), valid_labels))\n",
      "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
      "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
      "  #print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Initialized\n",
        "Minibatch loss at step 0: 2245802.500000\n",
        "Minibatch RMSE: 38.693687\n",
        "Validation RMSE: 665.562826\n",
        "Minibatch accuracy: 5.0%\n",
        "Validation accuracy: 63.6%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 100: 1346579.125000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch RMSE: 29.961967\n",
        "Validation RMSE: 528.913686\n",
        "Minibatch accuracy: 85.0%\n",
        "Validation accuracy: 85.5%\n",
        "Minibatch loss at step 200: 1251640.250000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch RMSE: 28.886446\n",
        "Validation RMSE: 447.316268\n",
        "Minibatch accuracy: 86.0%\n",
        "Validation accuracy: 86.1%\n",
        "Minibatch loss at step 300: 1358854.000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch RMSE: 30.098216\n",
        "Validation RMSE: 396.677845\n",
        "Minibatch accuracy: 88.0%\n",
        "Validation accuracy: 85.8%\n",
        "Minibatch loss at step 400: 1244435.250000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch RMSE: 28.803185\n",
        "Validation RMSE: 356.208682\n",
        "Minibatch accuracy: 90.0%\n",
        "Validation accuracy: 83.5%\n",
        "Minibatch loss at step 500: 1398408.000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch RMSE: 30.533127\n",
        "Validation RMSE: 323.466696\n",
        "Minibatch accuracy: 86.0%\n",
        "Validation accuracy: 83.2%\n",
        "Minibatch loss at step 600: 1374838.500000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch RMSE: 30.274730\n",
        "Validation RMSE: 298.351382\n",
        "Minibatch accuracy: 86.0%\n",
        "Validation accuracy: 82.4%\n",
        "Minibatch loss at step 700: 1225669.750000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch RMSE: 28.585191\n",
        "Validation RMSE: 278.468103\n",
        "Minibatch accuracy: 86.0%\n",
        "Validation accuracy: 83.0%\n",
        "Minibatch loss at step 800: 1325462.125000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch RMSE: 29.726110\n",
        "Validation RMSE: 268.737761\n",
        "Minibatch accuracy: 86.0%\n",
        "Validation accuracy: 83.0%\n",
        "Minibatch loss at step 900: 1307467.500000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch RMSE: 29.523631\n",
        "Validation RMSE: 255.514035\n",
        "Minibatch accuracy: 88.0%\n",
        "Validation accuracy: 83.5%\n",
        "Minibatch loss at step 1000: 1250738.375000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch RMSE: 28.876038\n",
        "Validation RMSE: 245.219951\n",
        "Minibatch accuracy: 89.0%\n",
        "Validation accuracy: 83.8%\n",
        "Minibatch loss at step 1100: 1380528.875000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch RMSE: 30.337314\n",
        "Validation RMSE: 234.661675\n",
        "Minibatch accuracy: 79.0%\n",
        "Validation accuracy: 84.1%\n"
       ]
      }
     ],
     "prompt_number": 90
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}